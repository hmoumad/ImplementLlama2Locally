## install Llama-2
To install Hugging Face locally, follow these steps:

1. Navigate to the Hugging Face model repository at the following link: TheBloke/Llama-2-7B-Chat-GGUF.

2. Once on the repository page, you'll find the model's details and information about its usage. Take a moment to review the documentation to familiarize yourself with the model.

3. Next, select the version of the model that best fits your requirements. You'll typically find multiple versions available, each with its own features and capabilities. Choose the version that aligns with your project needs and objectives.

4. Once you've selected a version, follow the installation instructions provided in the repository. This typically involves using the Hugging Face Transformers library to install the model locally on your system. You can install the model directly using pip or through a virtual environment, depending on your preferences and project setup.

5. After the installation is complete, you'll be ready to use the Hugging Face model locally in your projects. Import the model into your Python code and start leveraging its powerful natural language processing capabilities for various applications, such as text generation, sentiment analysis, and more.

## create virtual envirement using Conda

```conda create --name LlamaCpu python=3.11.4```<br>

```conda activate LlamaCpu```

## installation requirements

```pip install -U langchain```<br>

```pip install -U llama-cpp-python``` <br>

```pip install streamlit```
